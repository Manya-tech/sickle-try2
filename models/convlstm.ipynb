{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Taken from https://github.com/TUM-LMF/MTLCC-pytorch/blob/master/src/models/convlstm/convlstm.py<br>\n", "authors: TUM-LMF<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch.nn as nn\n", "from torch.autograd import Variable\n", "import torch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ConvLSTMCell(nn.Module):\n", "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n", "        \"\"\"\n", "        Initialize ConvLSTM cell.\n", "        Parameters\n", "        ----------\n", "        input_size: (int, int)\n", "            Height and width of input tensor as (height, width).\n", "        input_dim: int\n", "            Number of channels of input tensor.\n", "        hidden_dim: int\n", "            Number of channels of hidden state.\n", "        kernel_size: (int, int)\n", "            Size of the convolutional kernel.\n", "        bias: bool\n", "            Whether or not to add the bias.\n", "        \"\"\"\n", "        super(ConvLSTMCell, self).__init__()\n", "        self.height, self.width = input_size\n", "        self.input_dim = input_dim\n", "        self.hidden_dim = hidden_dim\n", "        self.kernel_size = kernel_size\n", "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n", "        self.bias = bias\n", "        self.conv = nn.Conv2d(\n", "            in_channels=self.input_dim + self.hidden_dim,\n", "            out_channels=4 * self.hidden_dim,\n", "            kernel_size=self.kernel_size,\n", "            padding=self.padding,\n", "            bias=self.bias,\n", "        )\n", "    def forward(self, input_tensor, cur_state):\n", "        h_cur, c_cur = cur_state\n", "        combined = torch.cat(\n", "            [input_tensor, h_cur], dim=1\n", "        )  # concatenate along channel axis\n", "        combined_conv = self.conv(combined)\n", "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n", "        i = torch.sigmoid(cc_i)\n", "        f = torch.sigmoid(cc_f)\n", "        o = torch.sigmoid(cc_o)\n", "        g = torch.tanh(cc_g)\n", "        c_next = f * c_cur + i * g\n", "        h_next = o * torch.tanh(c_next)\n", "        return h_next, c_next\n", "    def init_hidden(self, batch_size, device):\n", "        return (\n", "            Variable(\n", "                torch.zeros(batch_size, self.hidden_dim, self.height, self.width)\n", "            ).to(device),\n", "            Variable(\n", "                torch.zeros(batch_size, self.hidden_dim, self.height, self.width)\n", "            ).to(device),\n", "        )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ConvLSTM(nn.Module):\n", "    def __init__(\n", "        self,\n", "        input_size,\n", "        input_dim,\n", "        hidden_dim,\n", "        kernel_size,\n", "        num_layers=1,\n", "        batch_first=True,\n", "        bias=True,\n", "        return_all_layers=False,\n", "    ):\n", "        super(ConvLSTM, self).__init__()\n", "        self._check_kernel_size_consistency(kernel_size)\n\n", "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n", "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n", "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n", "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n", "            raise ValueError(\"Inconsistent list length.\")\n", "        self.height, self.width = input_size\n", "        self.input_dim = input_dim\n", "        self.hidden_dim = hidden_dim\n", "        self.kernel_size = kernel_size\n", "        self.num_layers = num_layers\n", "        self.batch_first = batch_first\n", "        self.bias = bias\n", "        self.return_all_layers = return_all_layers\n", "        cell_list = []\n", "        for i in range(0, self.num_layers):\n", "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n", "            cell_list.append(\n", "                ConvLSTMCell(\n", "                    input_size=(self.height, self.width),\n", "                    input_dim=cur_input_dim,\n", "                    hidden_dim=self.hidden_dim[i],\n", "                    kernel_size=self.kernel_size[i],\n", "                    bias=self.bias,\n", "                )\n", "            )\n", "        self.cell_list = nn.ModuleList(cell_list)\n", "    def forward(self, input_tensor, hidden_state=None, pad_mask=None):\n", "        \"\"\"\n", "        Parameters\n", "        ----------\n", "        input_tensor: todo\n", "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n", "        hidden_state: todo\n", "            None. todo implement stateful\n", "        pad_maks (b , t)\n", "        Returns\n", "        -------\n", "        last_state_list, layer_output\n", "        \"\"\"\n", "        if not self.batch_first:\n", "            # (t, b, c, h, w) -> (b, t, c, h, w)\n", "            input_tensor.permute(1, 0, 2, 3, 4)\n\n", "        # Implement stateful ConvLSTM\n", "        if hidden_state is not None:\n", "            raise NotImplementedError()\n", "        else:\n", "            hidden_state = self._init_hidden(\n", "                batch_size=input_tensor.size(0), device=input_tensor.device\n", "            )\n", "        layer_output_list = []\n", "        last_state_list = []\n", "        seq_len = input_tensor.size(1)\n", "        cur_layer_input = input_tensor\n", "        for layer_idx in range(self.num_layers):\n", "            h, c = hidden_state[layer_idx]\n", "            output_inner = []\n", "            for t in range(seq_len):\n", "                h, c = self.cell_list[layer_idx](\n", "                    input_tensor=cur_layer_input[:, t, :, :, :], cur_state=[h, c]\n", "                )\n", "                output_inner.append(h)\n", "            layer_output = torch.stack(output_inner, dim=1)\n", "            if pad_mask is not None:\n", "                last_positions = (~pad_mask).sum(dim=1) - 1\n", "                layer_output = layer_output[:, last_positions, :, :, :]\n", "            cur_layer_input = layer_output\n", "            layer_output_list.append(layer_output)\n", "            last_state_list.append([h, c])\n", "        if not self.return_all_layers:\n", "            layer_output_list = layer_output_list[-1:]\n", "            last_state_list = last_state_list[-1:]\n", "        return layer_output_list, last_state_list\n", "    def _init_hidden(self, batch_size, device):\n", "        init_states = []\n", "        for i in range(self.num_layers):\n", "            init_states.append(self.cell_list[i].init_hidden(batch_size, device))\n", "        return init_states\n", "    @staticmethod\n", "    def _check_kernel_size_consistency(kernel_size):\n", "        if not (\n", "            isinstance(kernel_size, tuple)\n", "            or (\n", "                isinstance(kernel_size, list)\n", "                and all([isinstance(elem, tuple) for elem in kernel_size])\n", "            )\n", "        ):\n", "            raise ValueError(\"`kernel_size` must be tuple or list of tuples\")\n", "    @staticmethod\n", "    def _extend_for_multilayer(param, num_layers):\n", "        if not isinstance(param, list):\n", "            param = [param] * num_layers\n", "        return param"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ConvLSTM_Seg(nn.Module):\n", "    def __init__(\n", "        self, num_classes, input_size, input_dim, hidden_dim, kernel_size, pad_value=0\n", "    ):\n", "        super(ConvLSTM_Seg, self).__init__()\n", "        self.convlstm_encoder = ConvLSTM(\n", "            input_dim=input_dim,\n", "            input_size=input_size,\n", "            hidden_dim=hidden_dim,\n", "            kernel_size=kernel_size,\n", "            return_all_layers=False,\n", "        )\n", "        self.classification_layer = nn.Conv2d(\n", "            in_channels=hidden_dim,\n", "            out_channels=num_classes,\n", "            kernel_size=kernel_size,\n", "            padding=1,\n", "        )\n", "        self.pad_value = pad_value\n", "    def forward(self, input, batch_positions=None):\n", "        pad_mask = (\n", "            (input == self.pad_value).all(dim=-1).all(dim=-1).all(dim=-1)\n", "        )  # BxT pad mask\n", "        pad_mask = pad_mask if pad_mask.any() else None\n", "        _, states = self.convlstm_encoder(input, pad_mask=pad_mask)\n", "        out = states[0][1]  # take last cell state as embedding\n", "        out = self.classification_layer(out)\n", "        return out"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BConvLSTM_Seg(nn.Module):\n", "    def __init__(\n", "        self, num_classes, input_size, input_dim, hidden_dim, kernel_size, pad_value=0\n", "    ):\n", "        super(BConvLSTM_Seg, self).__init__()\n", "        self.convlstm_forward = ConvLSTM(\n", "            input_dim=input_dim,\n", "            input_size=input_size,\n", "            hidden_dim=hidden_dim,\n", "            kernel_size=kernel_size,\n", "            return_all_layers=False,\n", "        )\n", "        self.convlstm_backward = ConvLSTM(\n", "            input_dim=input_dim,\n", "            input_size=input_size,\n", "            hidden_dim=hidden_dim,\n", "            kernel_size=kernel_size,\n", "            return_all_layers=False,\n", "        )\n", "        self.classification_layer = nn.Conv2d(\n", "            in_channels=2 * hidden_dim,\n", "            out_channels=num_classes,\n", "            kernel_size=kernel_size,\n", "            padding=1,\n", "        )\n", "        self.pad_value = pad_value\n", "    def forward(self, input, batch_posistions=None):\n", "        pad_mask = (\n", "            (input == self.pad_value).all(dim=-1).all(dim=-1).all(dim=-1)\n", "        )  # BxT pad mask\n", "        pad_mask = pad_mask if pad_mask.any() else None\n\n", "        # FORWARD\n", "        _, forward_states = self.convlstm_forward(input, pad_mask=pad_mask)\n", "        out = forward_states[0][1]  # take last cell state as embedding\n\n", "        # BACKWARD\n", "        x_reverse = torch.flip(input, dims=[1])\n", "        if pad_mask is not None:\n", "            pmr = torch.flip(pad_mask.float(), dims=[1]).bool()\n", "            x_reverse = torch.masked_fill(x_reverse, pmr[:, :, None, None, None], 0)\n", "            # Fill leading padded positions with 0s\n", "        _, backward_states = self.convlstm_backward(x_reverse)\n", "        out = torch.cat([out, backward_states[0][1]], dim=1)\n", "        out = self.classification_layer(out)\n", "        return out"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class BConvLSTM(nn.Module):\n", "    def __init__(self, input_size, input_dim, hidden_dim, kernel_size):\n", "        super(BConvLSTM, self).__init__()\n", "        self.convlstm_forward = ConvLSTM(\n", "            input_dim=input_dim,\n", "            input_size=input_size,\n", "            hidden_dim=hidden_dim,\n", "            kernel_size=kernel_size,\n", "            return_all_layers=False,\n", "        )\n", "        self.convlstm_backward = ConvLSTM(\n", "            input_dim=input_dim,\n", "            input_size=input_size,\n", "            hidden_dim=hidden_dim,\n", "            kernel_size=kernel_size,\n", "            return_all_layers=False,\n", "        )\n", "    def forward(self, input, pad_mask=None):\n", "        # FORWARD\n", "        _, forward_states = self.convlstm_forward(input, pad_mask=pad_mask)\n", "        out = forward_states[0][1]  # take last cell state as embedding\n\n", "        # BACKWARD\n", "        x_reverse = torch.flip(input, dims=[1])\n", "        if pad_mask is not None:\n", "            pmr = torch.flip(pad_mask.float(), dims=[1]).bool()\n", "            x_reverse = torch.masked_fill(x_reverse, pmr[:, :, None, None, None], 0)\n", "            # Fill leading padded positions with 0s\n", "        _, backward_states = self.convlstm_backward(x_reverse)\n", "        out = torch.cat([out, backward_states[0][1]], dim=1)\n", "        return out"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}