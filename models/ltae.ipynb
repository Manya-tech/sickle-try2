{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import copy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import torch\n", "import torch.nn as nn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from models.positional_encoding import PositionalEncoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class LTAE2d(nn.Module):\n", "    def __init__(\n", "        self,\n", "        in_channels=128,\n", "        n_head=16,\n", "        d_k=4,\n", "        mlp=[256, 128],\n", "        dropout=0.2,\n", "        d_model=256,\n", "        T=1000,\n", "        return_att=False,\n", "        positional_encoding=True,\n", "    ):\n", "        \"\"\"\n", "        Lightweight Temporal Attention Encoder (L-TAE) for image time series.\n", "        Attention-based sequence encoding that maps a sequence of images to a single feature map.\n", "        A shared L-TAE is applied to all pixel positions of the image sequence.\n", "        Args:\n", "            in_channels (int): Number of channels of the input embeddings.\n", "            n_head (int): Number of attention heads.\n", "            d_k (int): Dimension of the key and query vectors.\n", "            mlp (List[int]): Widths of the layers of the MLP that processes the concatenated outputs of the attention heads.\n", "            dropout (float): dropout\n", "            d_model (int, optional): If specified, the input tensors will first processed by a fully connected layer\n", "                to project them into a feature space of dimension d_model.\n", "            T (int): Period to use for the positional encoding.\n", "            return_att (bool): If true, the module returns the attention masks along with the embeddings (default False)\n", "            positional_encoding (bool): If False, no positional encoding is used (default True).\n", "        \"\"\"\n", "        super(LTAE2d, self).__init__()\n", "        self.in_channels = in_channels\n", "        self.mlp = copy.deepcopy(mlp)\n", "        self.return_att = return_att\n", "        self.n_head = n_head\n", "        if d_model is not None:\n", "            self.d_model = d_model\n", "            self.inconv = nn.Conv1d(in_channels, d_model, 1)\n", "        else:\n", "            self.d_model = in_channels\n", "            self.inconv = None\n", "        assert self.mlp[0] == self.d_model\n", "        if positional_encoding:\n", "            self.positional_encoder = PositionalEncoder(\n", "                self.d_model // n_head, T=T, repeat=n_head\n", "            )\n", "        else:\n", "            self.positional_encoder = None\n", "        self.attention_heads = MultiHeadAttention(\n", "            n_head=n_head, d_k=d_k, d_in=self.d_model\n", "        )\n", "        self.in_norm = nn.GroupNorm(\n", "            num_groups=n_head,\n", "            num_channels=self.in_channels,\n", "        )\n", "        self.out_norm = nn.GroupNorm(\n", "            num_groups=n_head,\n", "            num_channels=mlp[-1],\n", "        )\n", "        layers = []\n", "        for i in range(len(self.mlp) - 1):\n", "            layers.extend(\n", "                [\n", "                    nn.Linear(self.mlp[i], self.mlp[i + 1]),\n", "                    nn.BatchNorm1d(self.mlp[i + 1]),\n", "                    nn.ReLU(),\n", "                ]\n", "            )\n", "        self.mlp = nn.Sequential(*layers)\n", "        self.dropout = nn.Dropout(dropout)\n", "    def forward(self, x, batch_positions=None, pad_mask=None, return_comp=False):\n", "        # print(\"LTAE\",x.shape)\n", "        sz_b, seq_len, d, h, w = x.shape\n", "        if pad_mask is not None:\n", "            pad_mask = (\n", "                pad_mask.unsqueeze(-1)\n", "                .repeat((1, 1, h))\n", "                .unsqueeze(-1)\n", "                .repeat((1, 1, 1, w))\n", "            )  # BxTxHxW\n", "            pad_mask = (\n", "                pad_mask.permute(0, 2, 3, 1).contiguous().view(sz_b * h * w, seq_len)\n", "            )\n", "        out = x.permute(0, 3, 4, 1, 2).contiguous().view(sz_b * h * w, seq_len, d)\n", "        out = self.in_norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n", "        if self.inconv is not None:\n", "            out = self.inconv(out.permute(0, 2, 1)).permute(0, 2, 1)\n", "        if self.positional_encoder is not None:\n", "            # print(batch_positions)\n", "            bp = (\n", "                batch_positions.unsqueeze(-1)\n", "                .repeat((1, 1, h))\n", "                .unsqueeze(-1)\n", "                .repeat((1, 1, 1, w))\n", "            )  # BxTxHxW\n", "            # print(torch.unique(bp))\n", "            bp = bp.permute(0, 2, 3, 1).contiguous().view(sz_b * h * w, seq_len)\n", "            out = out + self.positional_encoder(bp)\n", "        out, attn = self.attention_heads(out, pad_mask=pad_mask)\n", "        out = (\n", "            out.permute(1, 0, 2).contiguous().view(sz_b * h * w, -1)\n", "        )  # Concatenate heads\n", "        out = self.dropout(self.mlp(out))\n", "        out = self.out_norm(out) if self.out_norm is not None else out\n", "        out = out.view(sz_b, h, w, -1).permute(0, 3, 1, 2)\n", "        attn = attn.view(self.n_head, sz_b, h, w, seq_len).permute(\n", "            0, 1, 4, 2, 3\n", "        )  # head x b x t x h x w\n", "        if self.return_att:\n", "            return out, attn\n", "        else:\n", "            return out"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MultiHeadAttention(nn.Module):\n", "    \"\"\"Multi-Head Attention module\n", "    Modified from github.com/jadore801120/attention-is-all-you-need-pytorch\n", "    \"\"\"\n", "    def __init__(self, n_head, d_k, d_in):\n", "        super().__init__()\n", "        self.n_head = n_head\n", "        self.d_k = d_k\n", "        self.d_in = d_in\n", "        self.Q = nn.Parameter(torch.zeros((n_head, d_k))).requires_grad_(True)\n", "        nn.init.normal_(self.Q, mean=0, std=np.sqrt(2.0 / (d_k)))\n", "        self.fc1_k = nn.Linear(d_in, n_head * d_k)\n", "        nn.init.normal_(self.fc1_k.weight, mean=0, std=np.sqrt(2.0 / (d_k)))\n", "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n", "    def forward(self, v, pad_mask=None, return_comp=False):\n", "        d_k, d_in, n_head = self.d_k, self.d_in, self.n_head\n", "        sz_b, seq_len, _ = v.size()\n", "        q = torch.stack([self.Q for _ in range(sz_b)], dim=1).view(\n", "            -1, d_k\n", "        )  # (n*b) x d_k\n", "        k = self.fc1_k(v).view(sz_b, seq_len, n_head, d_k)\n", "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, seq_len, d_k)  # (n*b) x lk x dk\n", "        if pad_mask is not None:\n", "            pad_mask = pad_mask.repeat(\n", "                (n_head, 1)\n", "            )  # replicate pad_mask for each head (nxb) x lk\n", "        v = torch.stack(v.split(v.shape[-1] // n_head, dim=-1)).view(\n", "            n_head * sz_b, seq_len, -1\n", "        )\n", "        if return_comp:\n", "            output, attn, comp = self.attention(\n", "                q, k, v, pad_mask=pad_mask, return_comp=return_comp\n", "            )\n", "        else:\n", "            output, attn = self.attention(\n", "                q, k, v, pad_mask=pad_mask, return_comp=return_comp\n", "            )\n", "        attn = attn.view(n_head, sz_b, 1, seq_len)\n", "        attn = attn.squeeze(dim=2)\n", "        output = output.view(n_head, sz_b, 1, d_in // n_head)\n", "        output = output.squeeze(dim=2)\n", "        if return_comp:\n", "            return output, attn, comp\n", "        else:\n", "            return output, attn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ScaledDotProductAttention(nn.Module):\n", "    \"\"\"Scaled Dot-Product Attention\n", "    Modified from github.com/jadore801120/attention-is-all-you-need-pytorch\n", "    \"\"\"\n", "    def __init__(self, temperature, attn_dropout=0.1):\n", "        super().__init__()\n", "        self.temperature = temperature\n", "        self.dropout = nn.Dropout(attn_dropout)\n", "        self.softmax = nn.Softmax(dim=2)\n", "    def forward(self, q, k, v, pad_mask=None, return_comp=False):\n", "        attn = torch.matmul(q.unsqueeze(1), k.transpose(1, 2))\n", "        attn = attn / self.temperature\n", "        if pad_mask is not None:\n", "            attn = attn.masked_fill(pad_mask.unsqueeze(1), -1e3)\n", "        if return_comp:\n", "            comp = attn\n", "        # compat = attn\n", "        attn = self.softmax(attn)\n", "        attn = self.dropout(attn)\n", "        output = torch.matmul(attn, v)\n", "        if return_comp:\n", "            return output, attn, comp\n", "        else:\n", "            return output, attn"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}