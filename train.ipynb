{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import wandb\n",
    "import pprint\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom import\n",
    "from utils.dataset import SICKLE_Dataset\n",
    "from utils import utae_utils, model_utils\n",
    "from utils.weight_init import weight_init\n",
    "from utils.metric import get_metrics, RMSELoss\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchnet as tnt\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Model parameters\n",
    "parser.add_argument(\n",
    "    \"--model\",\n",
    "    default=\"utae\",\n",
    "    type=str,\n",
    "    help=\"Type of architecture to use. Can be one of: (utae/unet3d/fpn/convlstm/convgru/uconvlstm/buconvlstm)\",\n",
    ")\n",
    "## U-TAE Hyperparameters\n",
    "parser.add_argument(\"--encoder_widths\", default=\"[64,128]\", type=str)\n",
    "parser.add_argument(\"--decoder_widths\", default=\"[32,128]\", type=str)\n",
    "parser.add_argument(\"--out_conv\", default=\"[32, 16]\")\n",
    "parser.add_argument(\"--str_conv_k\", default=4, type=int)\n",
    "parser.add_argument(\"--str_conv_s\", default=2, type=int)\n",
    "parser.add_argument(\"--str_conv_p\", default=1, type=int)\n",
    "parser.add_argument(\"--agg_mode\", default=\"att_group\", type=str)\n",
    "parser.add_argument(\"--encoder_norm\", default=\"group\", type=str)\n",
    "parser.add_argument(\"--n_head\", default=16, type=int)\n",
    "parser.add_argument(\"--d_model\", default=256, type=int)\n",
    "parser.add_argument(\"--d_k\", default=4, type=int)\n",
    "\n",
    "# Set-up parameters\n",
    "parser.add_argument(\n",
    "    \"--device\",\n",
    "    default= \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    type=str,\n",
    "    help=\"Name of device to use for tensor computations (cuda/cpu)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\", default=8, type=int, help=\"Number of data loading workers\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", default=0, type=int, help=\"Random seed\")\n",
    "# Training parameters\n",
    "parser.add_argument(\"--epochs\", default=100, type=int, help=\"Number of epochs per fold\")\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size\")\n",
    "parser.add_argument(\"--lr\", default=1e-1, type=float, help=\"Learning rate\")\n",
    "# parser.add_argument(\"--wd\", default=1e-2, type=float, help=\"weight decay\")\n",
    "parser.add_argument(\"--num_classes\", default=2, type=int)\n",
    "parser.add_argument(\"--ignore_index\", default=-999, type=int)\n",
    "parser.add_argument(\"--pad_value\", default=0, type=float)\n",
    "parser.add_argument(\"--padding_mode\", default=\"reflect\", type=str)\n",
    "parser.add_argument(\"--resume\", default=\"\", type=str, help=\"enter run path to resume\")\n",
    "parser.add_argument(\"--run_id\", default=\"\", type=str, help=\"enter run id to resume\")\n",
    "parser.add_argument(\"--wandb\", action='store_true', help=\"debug?\")\n",
    "parser.add_argument('--satellites', type=str, default=\"[S2]\")\n",
    "parser.add_argument('--run_name', type=str, default=\"trial\")\n",
    "parser.add_argument('--exp_name', type=str, default=\"utae\")\n",
    "parser.add_argument('--task', type=str, default=\"crop_type\",\n",
    "                    help=\"Available Tasks are crop_type, sowing_date, transplanting_date, harvesting_date, crop_yield\")\n",
    "parser.add_argument('--actual_season', action='store_true', help=\"whether to consider actual season or not.\")\n",
    "parser.add_argument('--data_dir', type=str, default=\"../sickle_dev/data\")\n",
    "parser.add_argument('--use_augmentation', type=bool, default=True)\n",
    "\n",
    "list_args = [\"encoder_widths\", \"decoder_widths\", \"out_conv\", \"satellites\"]\n",
    "parser.set_defaults(cache=False)\n",
    "\n",
    "\n",
    "def recursive_todevice(x, device):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device)\n",
    "    elif isinstance(x, dict):\n",
    "        return {k: recursive_todevice(v, device) for k, v in x.items()}\n",
    "    else:\n",
    "        return [recursive_todevice(c, device) for c in x]\n",
    "\n",
    "\n",
    "def prepare_output(CFG):\n",
    "    if CFG.wandb:\n",
    "        if not os.path.exists(CFG.run_path):\n",
    "            os.makedirs(CFG.run_path)\n",
    "        elif CFG.resume:\n",
    "            pass\n",
    "        else:\n",
    "            CFG.run_path = CFG.run_path + f\"_{time.time()}\"\n",
    "            print(\"Run path already exist changed run path to \", CFG.run_path)\n",
    "            os.makedirs(CFG.run_path)\n",
    "    else:\n",
    "        CFG.run_path += \"_debug\"\n",
    "        os.makedirs(CFG.run_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def checkpoint(log, config):\n",
    "    with open(\n",
    "            os.path.join(config.run_path, \"trainlog.json\"), \"w\"\n",
    "    ) as outfile:\n",
    "        json.dump(log, outfile, indent=4)\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # For reproducibility\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except Exception as e:\n",
    "        print(\"Can not use deterministic algorithm. Error: \", e)\n",
    "    print(f\"> SEEDING DONE {seed}\")\n",
    "\n",
    "\n",
    "def log_wandb(loss, metrics, table=None, phase=\"train\"):\n",
    "    f1_macro, acc, iou, f1_paddy, f1_non_paddy, \\\n",
    "    acc_paddy, acc_non_paddy, iou_paddy, iou_non_paddy, (y_pred, y_true) = metrics\n",
    "    y_pred, y_true = y_pred.tolist(), y_true.tolist()\n",
    "    if CFG.wandb:\n",
    "        log = {\n",
    "                f\"{phase}_loss\": loss,\n",
    "                f\"{phase}_f1_macro\": f1_macro,\n",
    "                f\"{phase}_acc\": acc,\n",
    "                f\"{phase}_iou\": iou,\n",
    "                f\"{phase}_f1_paddy\": f1_paddy,\n",
    "                f\"{phase}_f1_non_paddy\": f1_non_paddy,\n",
    "                f\"{phase}_acc_paddy\": acc_paddy,\n",
    "                f\"{phase}_acc_non_paddy\": acc_non_paddy,\n",
    "                f\"{phase}_iou_paddy\": iou_paddy,\n",
    "                f\"{phase}_iou_non_paddy\": iou_non_paddy,\n",
    "            }\n",
    "        if table is not None:\n",
    "            log[table[\"key\"]] = table[\"value\"]\n",
    "        wandb.log(log)\n",
    "        if phase == \"test\":\n",
    "            wandb.log({f\"{phase}_conf_mat\": wandb.plot.confusion_matrix(y_true=y_true, preds=y_pred, probs=None,\n",
    "                                                                        class_names=[\"Paddy\", \"Non Paddy\"])})\n",
    "\n",
    "\n",
    "def iterate(\n",
    "        model, data_loader, criterion, optimizer=None, scheduler=None, mode=\"train\", epoch=1, task=\"crop_type\",\n",
    "        device=None, log=False, CFG=None,\n",
    "):\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "    predictions = None\n",
    "    targets = None\n",
    "    pid_masks =None\n",
    "    if log:\n",
    "        columns = [\"image_l8\", \"image_s2\", \"image_s1\", \"gt_mask\", \"pred_filtered\", \"pred_whole\"]\n",
    "        wandb_table = wandb.Table(columns=columns)\n",
    "\n",
    "    t_start = time.time()\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=mode)\n",
    "    for i, batch in pbar:\n",
    "        if device is not None:\n",
    "            batch = recursive_todevice(batch, device)\n",
    "        data, masks = batch\n",
    "        plot_mask = masks[\"plot_mask\"]\n",
    "        masks = masks[task]\n",
    "        if task == \"crop_type\":\n",
    "            masks = masks.long()\n",
    "        else:\n",
    "            masks = masks.float()\n",
    "        if mode != \"train\":\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(data)\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(data)\n",
    "        if task==\"crop_yield\": \n",
    "            loss = criterion(y_pred, masks, plot_mask)\n",
    "        else:\n",
    "            loss = criterion(y_pred, masks)\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Compute Metric\n",
    "        if task == \"crop_type\":\n",
    "            y_pred = nn.Softmax(dim=1)(y_pred)\n",
    "\n",
    "        if predictions is None:\n",
    "            predictions = y_pred\n",
    "            targets = masks\n",
    "            pid_masks = plot_mask\n",
    "        else:\n",
    "            predictions = torch.cat([predictions, y_pred], dim=0)\n",
    "            targets = torch.cat([targets, masks], dim=0)\n",
    "            pid_masks = torch.cat([pid_masks, plot_mask], dim=0)\n",
    "            \n",
    "\n",
    "        if log:\n",
    "            if len(data.keys()) == 3:\n",
    "                (l8_images, l8_dates) = data[\"L8\"]\n",
    "                (s2_images, s2_dates) = data[\"S2\"]\n",
    "                (s1_images, s1_dates) = data[\"S1\"]\n",
    "            else:\n",
    "                (l8_images, l8_dates) = data[CFG.primary_sat]\n",
    "                (s2_images, s2_dates) = data[CFG.primary_sat]\n",
    "                (s1_images, s1_dates) = data[CFG.primary_sat]\n",
    "            if task == \"crop_type\":\n",
    "                # y_pred = torch.argmax(nn.Softmax(dim=1)(y_pred), dim=1)\n",
    "                y_pred = nn.Softmax(dim=1)(y_pred)[:, 0, :, :]\n",
    "                # log image of primary satellite\n",
    "                l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, y_pred, masks = \\\n",
    "                    l8_images.cpu().numpy(), s2_images.cpu().numpy(), s1_images.cpu().numpy(), \\\n",
    "                    l8_dates.cpu().numpy(), s2_dates.cpu().numpy(), s1_dates.cpu().numpy(), \\\n",
    "                    y_pred.cpu().numpy(), masks.cpu().numpy()\n",
    "            else:\n",
    "                # log image of primary satellite\n",
    "                y_pred = y_pred[:, 0, :, :]\n",
    "                l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, y_pred, masks = \\\n",
    "                    l8_images.cpu().numpy(), s2_images.cpu().numpy(), s1_images.cpu().numpy(), \\\n",
    "                    l8_dates.cpu().numpy(), s2_dates.cpu().numpy(), s1_dates.cpu().numpy(), \\\n",
    "                    y_pred.cpu().numpy(), masks.cpu().numpy()\n",
    "            log_test_predictions(l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, masks, y_pred, wandb_table, CFG=CFG)\n",
    "\n",
    "        loss_meter.add(loss.item())\n",
    "\n",
    "        # Just for Monitoring\n",
    "        mem = torch.cuda.memory_reserved() / 1e9 if torch.cuda.is_available() else 0\n",
    "        pbar.set_postfix(\n",
    "            Loss=f\"{loss.item():0.4f}\",\n",
    "            gpu_mem=f\"{mem:0.2f} GB\",\n",
    "        )\n",
    "    # take scheduler step\n",
    "    if scheduler is not None and epoch < 3 * CFG.epochs // 4:\n",
    "        scheduler.step()\n",
    "\n",
    "    t_end = time.time()\n",
    "    total_time = t_end - t_start\n",
    "    print(\"Epoch time : {:.1f}s\".format(total_time))\n",
    "    metrics = get_metrics(predictions, targets, pid_masks, ignore_index=CFG.ignore_index, task=task)\n",
    "    if log:\n",
    "        return loss_meter.value()[0], metrics, wandb_table\n",
    "    return loss_meter.value()[0], metrics\n",
    "\n",
    "\n",
    "n_log = 10  # no of samples to log\n",
    "\n",
    "def generate_heatmap(mask):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    fig = plt.figure()\n",
    "    hm = sns.heatmap(data=mask, vmin=-1, vmax=1 if np.max(mask) <= 1 else np.max(mask),\n",
    "                     cmap='RdYlGn')\n",
    "    plt.axis('off')\n",
    "    fig.canvas.draw()\n",
    "    mask = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    mask = mask.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def log_test_predictions(l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, gt_masks, pred_masks, test_table, CFG = None, task=\"crop_type\", ):\n",
    "    _id = 0\n",
    "    # print(gt_masks.shape,pred_masks.shape)\n",
    "    # pred_masks[pred_masks == 1] = 128\n",
    "    gt_masks[gt_masks == -999] = -1\n",
    "\n",
    "\n",
    "    # print(np.unique(pred_masks))\n",
    "    for l8_sample, s2_sample, s1_sample, l8_sample_dates, s2_sample_dates, s1_sample_dates, gt_mask, pred_mask in \\\n",
    "            zip(l8_images, s2_images, s1_images, l8_dates, s2_dates, s1_dates, gt_masks, pred_masks):\n",
    "        # get last available image\n",
    "        l8_image = l8_sample[len(l8_sample_dates[l8_sample_dates != 0]) - 1]\n",
    "        # reshape and normalize image\n",
    "        l8_image = l8_image[\n",
    "            CFG.satellites[\"L8\" if len(CFG.satellites) == 3 else CFG.primary_sat][\"rgb_bands\"]].transpose(1, 2, 0)\n",
    "        l8_image = ((l8_image - np.min(l8_image)) / (np.max(l8_image) - np.min(l8_image)))\n",
    "\n",
    "        s2_image = s2_sample[len(s2_sample_dates[s2_sample_dates != 0]) - 1]\n",
    "        # reshape and normalize image\n",
    "        s2_image = s2_image[\n",
    "            CFG.satellites[\"S2\" if len(CFG.satellites) == 3 else CFG.primary_sat][\"rgb_bands\"]].transpose(1, 2, 0)\n",
    "        s2_image = ((s2_image - np.min(s2_image)) / (np.max(s2_image) - np.min(s2_image)))\n",
    "\n",
    "        s1_image = s1_sample[len(s1_sample_dates[s1_sample_dates != 0]) - 1]\n",
    "        # reshape and normalize image\n",
    "        s1_image = s1_image[\n",
    "            CFG.satellites[\"S1\" if len(CFG.satellites) == 3 else CFG.primary_sat][\"rgb_bands\"]].transpose(1, 2, 0)\n",
    "        s1_image = ((s1_image - np.min(s1_image)) / (np.max(s1_image) - np.min(s1_image)))\n",
    "\n",
    "        # log whole prediction mask\n",
    "        pred_mask_whole = generate_heatmap(copy.deepcopy(pred_mask))\n",
    "        pred_mask[gt_mask == -1] = -1\n",
    "        pred_mask = generate_heatmap(copy.deepcopy(pred_mask))\n",
    "        if task == \"crop_type\":\n",
    "            gt_mask[gt_mask == 0] = 2\n",
    "            gt_mask[gt_mask == 1] = 0\n",
    "            gt_mask[gt_mask == 2] = 1\n",
    "        gt_mask = generate_heatmap(copy.deepcopy(gt_mask))\n",
    "\n",
    "        test_table.add_data(wandb.Image(l8_image), wandb.Image(s2_image), wandb.Image(s1_image), wandb.Image(gt_mask),\n",
    "                            wandb.Image(pred_mask), wandb.Image(pred_mask_whole))\n",
    "        _id += 1\n",
    "        if _id == n_log:\n",
    "            break\n",
    "\n",
    "\n",
    "def main(CFG):\n",
    "    prepare_output(CFG)\n",
    "    device = torch.device(CFG.device)\n",
    "\n",
    "    # Dataset definition\n",
    "    data_dir = CFG.data_dir\n",
    "    df = pd.read_csv(os.path.join(data_dir,\"sickle_dataset_tabular.csv\"))\n",
    "    # if \"S2\" in CFG.satellites.keys():\n",
    "    #     df = df[df[f\"S2_available\"] == True].reset_index(drop=True)\n",
    "    # else:\n",
    "    #     df = df[df[f\"{CFG.primary_sat}_available\"] == True].reset_index(drop=True)\n",
    "    if CFG.task != \"crop_type\":\n",
    "        df = df[df.YIELD > 0].reset_index(drop=True)\n",
    "\n",
    "    train_df = df[df.SPLIT == \"train\"].reset_index(drop=True)\n",
    "    val_df = df[df.SPLIT == \"val\"].reset_index(drop=True)\n",
    "    test_df = df[df.SPLIT == \"test\"].reset_index(drop=True)\n",
    "\n",
    "    dt_args = dict(\n",
    "        data_dir=data_dir,\n",
    "        satellites=CFG.satellites,\n",
    "        ignore_index=CFG.ignore_index,\n",
    "        transform=CFG.use_augmentation,\n",
    "        actual_season=CFG.actual_season\n",
    "    )\n",
    "\n",
    "    dt_train = SICKLE_Dataset(df=train_df, phase=\"train\", **dt_args)\n",
    "    dt_args = dict(\n",
    "        data_dir=data_dir,\n",
    "        satellites=CFG.satellites,\n",
    "        ignore_index=CFG.ignore_index,\n",
    "        actual_season=CFG.actual_season\n",
    "    )\n",
    "    dt_val = SICKLE_Dataset(df=val_df, **dt_args, )\n",
    "    dt_test = SICKLE_Dataset(df=test_df, **dt_args)\n",
    "\n",
    "    collate_fn = lambda x: utae_utils.pad_collate(x, pad_value=CFG.pad_value)\n",
    "    train_loader = data.DataLoader(\n",
    "        dt_train,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=CFG.num_workers,\n",
    "    )\n",
    "    val_loader = data.DataLoader(\n",
    "        dt_val,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=CFG.num_workers,\n",
    "    )\n",
    "    test_loader = data.DataLoader(\n",
    "        dt_test,\n",
    "        batch_size=CFG.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=CFG.num_workers,\n",
    "    )\n",
    "    batch_data, masks = next(iter(train_loader))\n",
    "    for sat in CFG.satellites.keys():\n",
    "        (samples, dates) = batch_data[sat]\n",
    "        print(f\"-----------{sat}------------\")\n",
    "        print(\"Samples Shape\", samples.shape, \"Masks Shape\", masks[\"crop_type\"].shape)\n",
    "        print(\"dates\", dates[0])\n",
    "        print(\"Samples\", torch.unique(samples[0]))\n",
    "        print(\"Masks\", torch.unique(masks[CFG.task]))\n",
    "\n",
    "    print(\n",
    "        \"Train {}, Val {}, Test {}\".format(len(dt_train), len(dt_val), len(dt_test))\n",
    "    )\n",
    "\n",
    "    # Model definition\n",
    "    # if len(CFG.satellites)==1:\n",
    "    #     print(\"Using Build model\")\n",
    "    #     model = model_utils.Build_model(CFG)\n",
    "    # else:\n",
    "    #     print(\"Using Fusion model\")\n",
    "    #     model = model_utils.Fusion_model(CFG)\n",
    "    model = model_utils.Fusion_model(CFG)\n",
    "    model.apply(weight_init)\n",
    "    model = model.to(device)\n",
    "    CFG.N_params = utae_utils.get_ntrainparams(model)\n",
    "    print(\"TOTAL TRAINABLE PARAMETERS :\", CFG.N_params)\n",
    "    with open(os.path.join(CFG.run_path, \"conf.json\"), \"w\") as file:\n",
    "        file.write(json.dumps(vars(CFG), indent=4))\n",
    "\n",
    "    # Optimizer, Loss and Scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\n",
    "    if CFG.task == \"crop_type\":\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=CFG.ignore_index,\n",
    "                                        weight=torch.tensor([0.62013, 0.37987])).to(device=CFG.device, dtype=torch.float32)\n",
    "    else:\n",
    "        criterion = RMSELoss(ignore_index=CFG.ignore_index)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=3 * CFG.epochs // 4, eta_min=1e-4)\n",
    "\n",
    "    # Training loop\n",
    "    trainlog = {}\n",
    "    best_metric = 0 if CFG.task == \"crop_type\" else torch.inf\n",
    "    for epoch in range(1, CFG.epochs + 1):\n",
    "        print(\"EPOCH {}/{}\".format(epoch, CFG.epochs))\n",
    "        model.train()\n",
    "        train_loss, train_metrics = iterate(\n",
    "            model,\n",
    "            data_loader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            mode=\"train\",\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "            task=CFG.task,\n",
    "            CFG=CFG,\n",
    "        )\n",
    "\n",
    "        print(\"Validation . . . \")\n",
    "        model.eval()\n",
    "        val_loss, val_metrics = iterate(\n",
    "            model,\n",
    "            data_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            mode=\"val\",\n",
    "            device=device,\n",
    "            task=CFG.task,\n",
    "            CFG=CFG,\n",
    "        )\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if CFG.task == \"crop_type\":\n",
    "            # train metrics\n",
    "            train_f1_macro, train_acc, train_iou, train_f1_paddy, train_f1_non_paddy, \\\n",
    "            train_acc_paddy, train_acc_non_paddy, train_iou_paddy, train_iou_non_paddy, _ = train_metrics\n",
    "            # val metric\n",
    "            val_f1_macro, val_acc, val_iou, val_f1_paddy, val_f1_non_paddy, \\\n",
    "            val_acc_paddy, val_acc_non_paddy, val_iou_paddy, val_iou_non_paddy, _ = val_metrics\n",
    "            deciding_metric = val_f1_macro\n",
    "            # log and print metrics\n",
    "            print(\n",
    "                f\"F1: {val_f1_macro:0.4f} | Paddy F1: {val_f1_paddy:0.4f} | Non-Paddy F1: {val_f1_non_paddy:0.4f} \\nAcc:{val_acc:0.4f} | Paddy Acc: {val_acc_paddy:0.4f} | Non-Paddy Acc: {val_acc_non_paddy:0.4f}\\niou:{val_iou:0.4f} | Paddy iou: {val_iou_paddy:0.4f} | Non-Paddy iou: {val_iou_non_paddy:0.4f}\")\n",
    "            trainlog[epoch] = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_f1\": train_f1_macro.item(),\n",
    "                \"train_f1_paddy\": train_f1_paddy.item(),\n",
    "                \"train_f1_non_paddy\": train_f1_non_paddy.item(),\n",
    "                \"train_acc\": train_acc.item(),\n",
    "                \"train_acc_paddy\": train_acc_paddy.item(),\n",
    "                \"train_acc_non_paddy\": train_acc_non_paddy.item(),\n",
    "                \"train_iou\": train_iou.item(),\n",
    "                \"train_iou_paddy\": train_iou_paddy.item(),\n",
    "                \"train_iou_non_paddy\": train_iou_non_paddy.item(),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_f1\": val_f1_macro.item(),\n",
    "                \"val_f1_paddy\": val_f1_paddy.item(),\n",
    "                \"val_f1_non_paddy\": val_f1_non_paddy.item(),\n",
    "                \"val_acc\": val_acc.item(),\n",
    "                \"val_acc_paddy\": val_acc_paddy.item(),\n",
    "                \"val_acc_non_paddy\": val_acc_non_paddy.item(),\n",
    "                \"val_iou\": val_iou.item(),\n",
    "                \"val_iou_paddy\": val_iou_paddy.item(),\n",
    "                \"val_iou_non_paddy\": val_iou_non_paddy.item(),\n",
    "                \"lr\": lr\n",
    "            }\n",
    "        else:\n",
    "            # train metrics\n",
    "            train_rmse, train_mae, train_mape = train_metrics\n",
    "            # val metrics\n",
    "            val_rmse, val_mae, val_mape = val_metrics\n",
    "            deciding_metric = val_mae\n",
    "            print(f\"Val RMSE: {val_rmse:0.4f} | Val MAE: {val_mae:0.4f} | Val MAPE: {val_mape:0.4f}\")\n",
    "            trainlog[epoch] = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_rmse\": train_rmse.item(),\n",
    "                \"train_mae\": train_mae.item(),\n",
    "                \"train_mape\": train_mape.item(),\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_rmse\": val_rmse.item(),\n",
    "                \"val_mae\": val_mae.item(),\n",
    "                \"val_mape\": val_mape.item(),\n",
    "                \"lr\": lr,\n",
    "            }\n",
    "\n",
    "        checkpoint(trainlog, CFG)\n",
    "        if CFG.wandb:\n",
    "            wandb.log(trainlog[epoch])\n",
    "\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"model\": model.state_dict()\n",
    "        }\n",
    "\n",
    "        if (deciding_metric > best_metric and CFG.task == \"crop_type\") or (\n",
    "                deciding_metric < best_metric and CFG.task != \"crop_type\"):\n",
    "            print(f\"Valid Score Improved ({best_metric:0.4f} ---> {deciding_metric:0.4f})\")\n",
    "            best_metric = deciding_metric\n",
    "            torch.save(\n",
    "                save_dict,\n",
    "                os.path.join(\n",
    "                    CFG.run_path, \"checkpoint_best.pth.tar\"\n",
    "                ),\n",
    "            )\n",
    "        torch.save(\n",
    "            save_dict,\n",
    "            os.path.join(\n",
    "                CFG.run_path, \"checkpoint_last.pth.tar\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    print(\"Testing best epoch . . .\")\n",
    "    best_checkpoint = torch.load(\n",
    "        os.path.join(\n",
    "            CFG.run_path, \"checkpoint_best.pth.tar\"\n",
    "        )\n",
    "    )\n",
    "    model.load_state_dict(best_checkpoint[\"model\"])\n",
    "    model.eval()\n",
    "    arg_dict = dict(\n",
    "        model=model,\n",
    "        data_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        mode=\"val\",\n",
    "        device=device,\n",
    "        task=CFG.task,\n",
    "    )\n",
    "\n",
    "    val_loss, val_metrics, wandb_table = iterate(log=True, **arg_dict)\n",
    "    print(\"Validation Result\")\n",
    "    if CFG.task == \"crop_type\":\n",
    "        # test metric\n",
    "        best_val_f1_macro, best_val_acc, best_val_iou, best_val_f1_paddy, best_val_f1_non_paddy, \\\n",
    "        best_val_acc_paddy, best_val_acc_non_paddy, best_val_iou_paddy, best_val_iou_non_paddy, _ = val_metrics\n",
    "        deciding_metric = best_val_f1_macro\n",
    "        # log and print metrics\n",
    "        print(\n",
    "            f\"F1: {best_val_f1_macro:0.4f} | Paddy F1: {best_val_f1_paddy:0.4f} | Non-Paddy F1: {best_val_f1_non_paddy:0.4f} \\nAcc:{best_val_acc:0.4f} | Paddy Acc: {best_val_acc_paddy:0.4f} | Non-Paddy Acc: {best_val_acc_non_paddy:0.4f}\\niou:{best_val_iou:0.4f} | Paddy iou: {best_val_iou_paddy:0.4f} | Non-Paddy iou: {best_val_iou_non_paddy:0.4f}\")\n",
    "\n",
    "    else:\n",
    "        # test metrics\n",
    "        best_val_rmse, best_val_mae, best_val_mape = val_metrics\n",
    "        print(f\"Test RMSE: {best_val_rmse:0.4f} | Test MAE: {best_val_mae:0.4f} | Test MAPE: {best_val_mape:0.4f}\")\n",
    "\n",
    "    if CFG.wandb:\n",
    "        wandb.log({f\"{CFG.primary_sat}_val_prediction\": wandb_table})\n",
    "\n",
    "    test_loss, test_metrics, wandb_table = iterate(\n",
    "        model,\n",
    "        data_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        mode=\"test\",\n",
    "        device=device,\n",
    "        task=CFG.task,\n",
    "        log=True\n",
    "    )\n",
    "    print(\"Test Result\")\n",
    "    if CFG.task == \"crop_type\":\n",
    "        # test metric\n",
    "        test_f1_macro, test_acc, test_iou, test_f1_paddy, test_f1_non_paddy, \\\n",
    "        test_acc_paddy, test_acc_non_paddy, test_iou_paddy, test_iou_non_paddy, _ = test_metrics\n",
    "        deciding_metric = test_f1_macro\n",
    "        # log and print metrics\n",
    "        print(\n",
    "            f\"F1: {test_f1_macro:0.4f} | Paddy F1: {test_f1_paddy:0.4f} | Non-Paddy F1: {test_f1_non_paddy:0.4f} \\nAcc:{test_acc:0.4f} | Paddy Acc: {test_acc_paddy:0.4f} | Non-Paddy Acc: {test_acc_non_paddy:0.4f}\\niou:{test_iou:0.4f} | Paddy iou: {test_iou_paddy:0.4f} | Non-Paddy iou: {test_iou_non_paddy:0.4f}\")\n",
    "        log_wandb(test_loss, test_metrics, {\"key\": f\"{CFG.primary_sat}_test_prediction\", \"value\": wandb_table}, phase=\"test\")\n",
    "\n",
    "    else:\n",
    "        # test metrics\n",
    "        test_rmse, test_mae, test_mape = test_metrics\n",
    "        print(f\"Test RMSE: {test_rmse:0.4f} | Test MAE: {test_mae:0.4f} | Test MAPE: {test_mape:0.4f}\")\n",
    "        testlog = {\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_rmse\": test_rmse.item(),\n",
    "            \"test_mae\": test_mae.item(),\n",
    "            \"test_mape\": test_mape.item(),\n",
    "            f\"{CFG.primary_sat}_test_prediction\": wandb_table,\n",
    "            \"lr\": lr,\n",
    "        }\n",
    "        if CFG.wandb:\n",
    "            wandb.log(testlog)\n",
    "    # log model to wandb \n",
    "    if CFG.wandb:\n",
    "        best = wandb.Artifact('checkpoint_best', type='model')\n",
    "        best.add_file(os.path.join(CFG.run_path, \"checkpoint_best.pth.tar\"))\n",
    "        last = wandb.Artifact('checkpoint_last', type='model')\n",
    "        last.add_file(os.path.join(CFG.run_path, \"checkpoint_last.pth.tar\"))\n",
    "        wandb.log_artifact(best)\n",
    "        wandb.log_artifact(last)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    CFG = parser.parse_args()\n",
    "    set_seed(CFG.seed)\n",
    "    for k, v in vars(CFG).items():\n",
    "        if k in list_args and v is not None:\n",
    "            v = v.replace(\"[\", \"\")\n",
    "            v = v.replace(\"]\", \"\")\n",
    "            try:\n",
    "                CFG.__setattr__(k, list(map(int, v.split(\",\"))))\n",
    "            except:\n",
    "                CFG.__setattr__(k, list(map(str, v.split(\",\"))))\n",
    "                \n",
    "    CFG.exp_name = CFG.task\n",
    "    \n",
    "    # if task type is regression. Increase lr and change output channel to 1 \n",
    "    if CFG.task != \"crop_type\":\n",
    "        # CFG.lr = 1e-1\n",
    "        CFG.num_classes = 1\n",
    "        # CFG.out_conv[-1] = 1\n",
    "        \n",
    "    # change out_conv incase of fusion\n",
    "    # if len(CFG.satellites) >1:\n",
    "    #     CFG.out_conv[-1] =  16\n",
    "    # else:\n",
    "    #     assert CFG.num_classes == CFG.out_conv[-1]\n",
    "        \n",
    "\n",
    "    CFG.run_path = f\"runs/wacv_2024_seed{CFG.seed}/{CFG.exp_name}/{CFG.run_name}\"\n",
    "    satellite_metadata = {\n",
    "        \"S2\": {\n",
    "            \"bands\": ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12'],\n",
    "            \"rgb_bands\": [3, 2, 1],\n",
    "            \"mask_res\": 10,\n",
    "            \"img_size\": (32, 32),\n",
    "        },\n",
    "        \"S1\": {\n",
    "            \"bands\": ['VV', 'VH'],\n",
    "            \"rgb_bands\": [0, 1, 0],\n",
    "            \"mask_res\": 10,\n",
    "            \"img_size\": (32, 32),\n",
    "        },\n",
    "        \"L8\": {\n",
    "            \"bands\": [\"SR_B1\", \"SR_B2\", \"SR_B3\", \"SR_B4\", \"SR_B5\", \"SR_B6\", \"SR_B7\", \"ST_B10\"],\n",
    "            \"rgb_bands\": [3, 2, 1],\n",
    "            \"mask_res\": 10,\n",
    "            \"img_size\": (32, 32),\n",
    "        },\n",
    "    }\n",
    "    required_sat_data = {}\n",
    "    for satellite in CFG.satellites:\n",
    "        required_sat_data[satellite] = satellite_metadata[satellite]\n",
    "    CFG.satellites = required_sat_data\n",
    "    # first satellie is primary, img_size and mask_res is decided by it\n",
    "    CFG.primary_sat =list(required_sat_data.keys())[0]\n",
    "    CFG.img_size = required_sat_data[CFG.primary_sat][\"img_size\"]\n",
    "\n",
    "    # WandB\n",
    "    if CFG.wandb:\n",
    "        wandb.login()\n",
    "        run = wandb.init(\n",
    "            project=f\"wacv_2024_seed{CFG.seed}\",\n",
    "            entity=\"agrifieldnet\",\n",
    "            config={k: v for k, v in dict(vars(CFG)).items() if \"__\" not in k},\n",
    "            name=CFG.run_name,\n",
    "            group=CFG.exp_name,\n",
    "        )\n",
    "\n",
    "    pprint.pprint(CFG)\n",
    "    main(CFG)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
